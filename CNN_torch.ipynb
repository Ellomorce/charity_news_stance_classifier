{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "HSJDsz0uO32v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "3SqFT8ytO_ob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27896600-8251-485a-fda6-0f29853e076e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dBcJRmv4PBKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb05ef3e-aa4c-4097-a2e4-52418ac973c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define File Path\n",
        "vec20avg_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec20_avg.npz\"\n",
        "vec25avg_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec25_avg.npz\"\n",
        "vec30avg_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec30_avg.npz\"\n",
        "vec35avg_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec35_avg.npz\"\n",
        "vec20sum_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec20_sum.npz\"\n",
        "vec20sum_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec20_sum.npz\"\n",
        "vec25sum_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec25_sum.npz\"\n",
        "vec30sum_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec30_sum.npz\"\n",
        "vec35sum_path = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/vec35_sum.npz\"\n",
        "freq_stance_labels = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/freq_stance_labels.npz\"\n",
        "oh_stance_labels = \"/content/drive/MyDrive/Co-working /Thesis/vec_data/oh_stance_labels.npz\""
      ],
      "metadata": {
        "id": "UvjEoMUkPCxH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_npz_file(filepath):\n",
        "    # Load the numpy array from the .npz file\n",
        "    with np.load(filepath, allow_pickle=True) as data:\n",
        "        for key in data.keys():\n",
        "            arr = data[key]\n",
        "            break\n",
        "    return arr"
      ],
      "metadata": {
        "id": "YwyIM8IrPEpL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec20avg = load_npz_file(vec20avg_path)\n",
        "vec25avg = load_npz_file(vec25avg_path)\n",
        "vec30avg = load_npz_file(vec30avg_path)\n",
        "vec35avg = load_npz_file(vec35avg_path)\n",
        "vec20sum = load_npz_file(vec20sum_path)\n",
        "vec20sum = load_npz_file(vec20sum_path)\n",
        "vec25sum = load_npz_file(vec25sum_path)\n",
        "vec30sum = load_npz_file(vec30sum_path)\n",
        "vec35sum = load_npz_file(vec35sum_path)\n",
        "freq_label = load_npz_file(freq_stance_labels)\n",
        "oh_label = load_npz_file(oh_stance_labels)"
      ],
      "metadata": {
        "id": "fCiAOnl0PGcB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#指派實際要使用的Data與Label\n",
        "data = vec30avg\n",
        "label = np.argmax(oh_label, axis=1)\n",
        "# label = freq_label\n",
        "print(data.shape)\n",
        "print(label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7g4vY4HRXPx",
        "outputId": "26ce7192-a95c-4d68-8541-c5297174e312"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(445, 1, 300)\n",
            "(445,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KC6EBhOb79f",
        "outputId": "f5758d5a-1b18-4f2e-8139-2d3f2cd4f1b6"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 0 0 1\n",
            " 1 2 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 2 1 1 2 2 2 1 1 1 0 0 0 2 2 1 1 1 1 1\n",
            " 1 1 1 1 1 2 1 1 1 2 2 1 2 1 2 1 2 1 2 2 1 2 2 2 1 1 2 1 1 1 1 1 2 1 1 2 2\n",
            " 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 1 2 1 1 2 1\n",
            " 2 1 1 1 1 1 1 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 2\n",
            " 1 1 2 1 1 1 2 2 2 2 1 1 2 2 1 2 2 2 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 0\n",
            " 0 0 0 0 0 1 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 2 1 1 1\n",
            " 2 1 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(data, label, test_size=0.2)\n",
        "print('Train data shape:', train_data.shape)\n",
        "print('Train labels shape:', train_labels.shape)\n",
        "print('Test data shape:', test_data.shape)\n",
        "print('Test labels shape:', test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a55-MfqoWNMz",
        "outputId": "f99b3ed2-27d6-445b-98ce-642d996c82e7"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (356, 1, 300)\n",
            "Train labels shape: (356,)\n",
            "Test data shape: (89, 1, 300)\n",
            "Test labels shape: (89,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtHhch_xgmwp",
        "outputId": "8e1b6464-cac6-4984-ccb3-43e72152e196"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-7.08233416e-02 -6.28900081e-02  3.91806632e-01  1.55733332e-01\n",
            "   8.57666414e-03  4.06366624e-02  4.04233299e-02 -5.65500036e-02\n",
            "   7.58366734e-02  1.15633318e-02  1.82499997e-02  6.20866641e-02\n",
            "  -3.12599987e-02  9.91666783e-03  8.74333363e-03 -2.41999817e-03\n",
            "   1.15166660e-02  2.26600040e-02 -3.59333167e-03  2.36233342e-02\n",
            "   8.41666386e-03 -1.17499996e-02  4.41100001e-02  2.83900034e-02\n",
            "   1.37300007e-02  3.12699974e-02 -3.10600046e-02 -1.21400002e-02\n",
            "   3.49766687e-02 -1.57866664e-02  6.04166612e-02 -1.89566687e-02\n",
            "   7.47766718e-02 -2.58799959e-02 -8.63566697e-02 -3.96566652e-02\n",
            "  -1.54866660e-02 -1.53846681e-01 -6.99666794e-03  4.81000356e-03\n",
            "   2.93199997e-02  4.32400033e-02 -4.33766730e-02  3.05566657e-02\n",
            "   5.84566630e-02  8.23000353e-03 -4.21976686e-01 -1.17333385e-03\n",
            "   3.34033370e-02 -8.78333859e-03  2.88866702e-02 -4.97166626e-02\n",
            "  -2.36333325e-03 -1.69893339e-01 -4.79966626e-02  3.36333271e-03\n",
            "   7.44600073e-02 -2.48433352e-02 -3.93966697e-02  1.02799991e-02\n",
            "  -5.94599992e-02 -8.34899992e-02 -2.86266673e-02 -6.02933317e-02\n",
            "   1.28466673e-02 -3.11699975e-02 -3.54366638e-02  3.82233337e-02\n",
            "   7.69133419e-02 -9.05000046e-03  7.21666496e-03  1.02383345e-01\n",
            "   2.86000129e-03  1.15499971e-02  6.98733404e-02  4.17300053e-02\n",
            "   2.27933340e-02 -2.42600013e-02  3.32533345e-02  3.32166702e-02\n",
            "  -5.57433255e-02  9.06666100e-04  4.54199947e-02  7.93666691e-02\n",
            "  -2.83666681e-02  4.66666883e-03  4.24633287e-02 -1.86333321e-02\n",
            "   1.21323332e-01 -3.28033231e-02  6.28466681e-02  1.66833345e-02\n",
            "  -2.30833348e-02 -3.49000213e-03 -1.89100020e-02 -2.47999979e-03\n",
            "   1.15966685e-02 -7.63866678e-02 -9.98833328e-02  1.32033331e-02\n",
            "  -2.40490019e-01  4.63366583e-02  2.08866615e-02  6.83199987e-02\n",
            "   8.94300044e-02 -4.26933356e-02 -1.93366688e-02  4.66000102e-03\n",
            "  -1.58933364e-02 -2.94300020e-02  5.10699935e-02 -2.61999983e-02\n",
            "  -5.38333319e-02  1.08576670e-01 -6.92666881e-03 -3.47799994e-02\n",
            "   2.11667013e-03  4.44799997e-02 -3.60766724e-02 -5.01299985e-02\n",
            "  -1.16180010e-01 -5.09999972e-03  7.64166787e-02  9.09998722e-04\n",
            "   9.08833295e-02 -3.17699984e-02 -4.27666679e-02 -1.29053324e-01\n",
            "  -2.57666549e-03  2.63499971e-02 -1.40676677e-01  5.83166666e-02\n",
            "   3.67666525e-03 -2.62333499e-03  5.99333318e-03  2.72066668e-02\n",
            "  -5.99033311e-02  1.47986665e-01 -2.50066686e-02  1.18433349e-02\n",
            "  -3.09299994e-02  1.35333324e-02 -2.18999963e-02  7.81999994e-03\n",
            "   7.29300007e-02  6.67333370e-03  2.84466669e-02 -1.33406654e-01\n",
            "   1.74666673e-03 -2.15333351e-03  6.81533366e-02  3.64533253e-02\n",
            "  -9.26300064e-02  2.21533366e-02  4.45667049e-03  3.42266597e-02\n",
            "  -1.02133332e-02 -1.24733364e-02  2.46566627e-02 -1.34230003e-01\n",
            "   9.98000149e-03  2.08733343e-02  1.84333324e-02 -1.18833333e-02\n",
            "  -3.19133364e-02  2.21066717e-02  4.51599993e-02  1.24533344e-02\n",
            "  -7.28466660e-02 -1.92499999e-02 -5.49833328e-02  4.37999936e-03\n",
            "  -8.28966647e-02  5.17333299e-02 -8.11533481e-02  2.36333329e-02\n",
            "  -2.88100000e-02  3.06566712e-02  4.94999904e-03 -3.16499993e-02\n",
            "  -2.82666646e-02  1.47666736e-03 -1.97999999e-02  4.87666531e-03\n",
            "   1.28733357e-02  7.74266571e-02  1.84433274e-02 -5.87433353e-02\n",
            "   5.64999972e-03  3.38653296e-01  4.02399972e-02  4.02200036e-02\n",
            "   4.37300019e-02 -5.53666754e-03  5.13667054e-03 -8.25433359e-02\n",
            "   3.72733399e-02 -2.10666656e-03  8.36066753e-02 -3.16099972e-02\n",
            "  -5.57300001e-02 -2.30600033e-02 -3.23000131e-03 -9.13666468e-03\n",
            "  -4.85000052e-02 -2.21999874e-03  1.85526684e-01  3.37399989e-02\n",
            "   3.46666686e-02  3.19000124e-03 -5.61500043e-02 -3.12766656e-02\n",
            "   2.46333377e-03  4.20000666e-04 -1.68366674e-02 -7.80666620e-02\n",
            "   6.08033426e-02 -1.10033331e-02 -2.85000000e-02 -3.51333269e-03\n",
            "  -1.17466664e-02  1.69400014e-02 -1.31366672e-02  4.01033349e-02\n",
            "  -1.76665562e-04  5.42200021e-02 -2.35633329e-02  2.83066668e-02\n",
            "   5.60333626e-03 -2.43773296e-01 -3.13033387e-02  6.77706778e-01\n",
            "   5.38133346e-02 -1.49833318e-02  3.34133357e-02 -2.02999953e-02\n",
            "   6.85866624e-02 -3.71666774e-02 -1.18366657e-02 -2.07599942e-02\n",
            "  -5.44200018e-02  2.91499998e-02 -8.94866660e-02 -2.72999960e-03\n",
            "   2.47799959e-02  3.14533301e-02 -6.02933280e-02 -2.78333332e-02\n",
            "  -1.68333314e-02  2.12299991e-02 -2.70633381e-02 -2.23266706e-02\n",
            "   5.57299964e-02  5.46666502e-04  5.32000186e-03  4.67833243e-02\n",
            "   4.90000099e-03 -3.38333324e-02  4.28933315e-02  1.10633336e-02\n",
            "   6.03466630e-02  4.59900014e-02  5.38199954e-02  1.25566684e-02\n",
            "  -5.59666287e-03  6.67999983e-02  1.80999911e-03  1.96133312e-02\n",
            "  -4.34633344e-02 -3.41399983e-02  2.36199964e-02 -8.80000356e-04\n",
            "   2.58466676e-02 -6.17666636e-03  3.05666681e-02 -8.99366662e-02\n",
            "  -1.22189999e-01 -4.43099998e-02 -1.85633339e-02 -1.04666478e-03\n",
            "  -1.14070006e-01 -3.68966646e-02 -7.32333167e-03  1.07256666e-01\n",
            "  -2.20899992e-02  1.15749985e-01 -5.41333370e-02  3.79666546e-03\n",
            "   5.94733357e-02 -9.73333418e-03 -3.31366621e-02  4.01233360e-02\n",
            "   6.76766708e-02  2.80999974e-03 -2.38766633e-02  5.89200035e-02\n",
            "  -1.27233332e-02  3.23533341e-02 -3.21766660e-02  7.56666414e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m25PesT9CRt6",
        "outputId": "19b15344-ff68-430e-9192-84962f71d50f"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data and labels to PyTorch tensors\n",
        "train_data_tensor = torch.from_numpy(train_data).float()\n",
        "train_labels_tensor = torch.from_numpy(train_labels).float()\n",
        "test_data_tensor = torch.from_numpy(test_data).float()\n",
        "test_labels_tensor = torch.from_numpy(test_labels).float()"
      ],
      "metadata": {
        "id": "2svkowiBXjci"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shk7yWXQe3xH",
        "outputId": "edb5c69d-311c-49cc-b16a-1f8b5613797d"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 2., 2., 2., 2., 3., 3., 1., 1., 3., 2., 1., 1., 1., 3., 1., 0.,\n",
            "        1., 3., 1., 1., 1., 0., 1., 1., 3., 1., 3., 3., 2., 3., 3., 3., 1., 3.,\n",
            "        1., 1., 3., 2., 3., 2., 1., 0., 1., 3., 1., 3., 1., 2., 1., 2., 1., 2.,\n",
            "        2., 1., 1., 1., 3., 2., 1., 3., 3., 3., 3., 1., 3., 2., 1., 2., 3., 3.,\n",
            "        3., 3., 3., 3., 1., 3., 1., 3., 1., 2., 2., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 2., 3., 2., 1., 3., 1., 3., 3., 3., 3., 2., 1., 1., 1., 1., 1., 2.,\n",
            "        3., 1., 2., 3., 3., 3., 1., 3., 3., 3., 3., 1., 2., 3., 1., 2., 2., 3.,\n",
            "        3., 3., 1., 1., 1., 3., 1., 2., 3., 1., 2., 1., 2., 1., 1., 1., 1., 2.,\n",
            "        2., 1., 1., 3., 1., 3., 3., 2., 2., 3., 2., 1., 3., 3., 1., 1., 1., 1.,\n",
            "        1., 2., 3., 1., 1., 1., 3., 3., 2., 1., 3., 1., 1., 3., 3., 1., 3., 1.,\n",
            "        3., 1., 1., 3., 1., 3., 1., 1., 2., 2., 3., 2., 2., 2., 1., 3., 3., 3.,\n",
            "        1., 3., 2., 3., 3., 1., 3., 3., 1., 3., 3., 2., 3., 3., 3., 2., 1., 1.,\n",
            "        3., 3., 1., 3., 1., 3., 1., 1., 1., 2., 1., 3., 2., 1., 3., 1., 3., 1.,\n",
            "        2., 1., 2., 1., 1., 1., 2., 2., 1., 1., 3., 3., 3., 3., 1., 2., 1., 2.,\n",
            "        1., 1., 3., 3., 3., 1., 1., 0., 1., 3., 1., 2., 1., 3., 1., 3., 1., 2.,\n",
            "        3., 3., 3., 1., 1., 1., 1., 1., 2., 1., 3., 1., 1., 3., 1., 2., 1., 0.,\n",
            "        3., 1., 2., 2., 1., 1., 2., 3., 2., 2., 3., 3., 1., 1., 3., 3., 1., 1.,\n",
            "        3., 2., 3., 1., 1., 3., 2., 1., 1., 3., 1., 2., 1., 3., 1., 1., 1., 2.,\n",
            "        2., 3., 1., 3., 1., 3., 3., 1., 3., 2., 1., 1., 1., 2., 2., 2., 1., 3.,\n",
            "        1., 3., 3., 3., 3., 3., 0., 0., 1., 3., 1., 3., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorDataset for each set of data and labels\n",
        "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)"
      ],
      "metadata": {
        "id": "CX77TLMrXm1n"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for each dataset to batch and shuffle the data\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "DdXI7GVvXpQE"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model architecture\n",
        "# class CNN(nn.Module):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(CNN, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "#         self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "#         self.fc1 = nn.Linear(in_features=2400, out_features=64)\n",
        "#         self.relu3 = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.relu1(x)\n",
        "#         x = self.pool1(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.relu2(x)\n",
        "#         x = self.pool2(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu3(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=5)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(64 * 73, 256)\n",
        "        self.fc2 = nn.Linear(256, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        print(x.shape)\n",
        "        x = x.view(-1, 64 * 73)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "u2OqRj8SXxei"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loss(model, loader, criterion):\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    return total_loss / total_samples"
      ],
      "metadata": {
        "id": "wAjpy9fbAYNV"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the CNN model\n",
        "num_classes = 4\n",
        "model = CNN()\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "g63AzXK9X7tW"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs to wait before stopping if the test accuracy does not improve\n",
        "patience = 3\n",
        "# Initialize the counter for the number of epochs without improvement\n",
        "counter = 0\n",
        "# Initialize the best test accuracy to zero\n",
        "best_val_accuracy = 0.0"
      ],
      "metadata": {
        "id": "O_MC5xGz_Uu6"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 3\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # Move the inputs and labels to the device\n",
        "        # inputs = inputs.to(device)\n",
        "        # labels = labels.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        # Compute the loss\n",
        "        print(f\"Batch {i} labels: {labels}\")\n",
        "        loss = criterion(outputs, labels.argmax(dim=-1))\n",
        "        # Backward pass and update the parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update the training loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Compute the training accuracy\n",
        "    train_predictions = []\n",
        "    train_true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            train_predictions.extend(predictions.tolist())\n",
        "            train_true_labels.extend(labels.argmax(dim=-1).tolist())\n",
        "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
        "\n",
        "    # Compute the test accuracy\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            val_predictions.extend(predictions.tolist())\n",
        "            val_true_labels.extend(labels.argmax(dim=-1).tolist())\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "\n",
        "    # Compute the training and test loss\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss = evaluate_loss(model, test_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Record the training and test accuracy\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Check if the test accuracy has improved\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        # Update the best test accuracy and reset the counter\n",
        "        best_val_accuracy = val_accuracy\n",
        "        counter = 0\n",
        "    else:\n",
        "        # Increment the counter\n",
        "        counter += 1\n",
        "        # Check if the counter has reached the patience limit\n",
        "        if counter == patience:\n",
        "            print(f\"Early stopping after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    # Print the epoch number, training loss, training accuracy, and test accuracy\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - Test Accuracy: {val_accuracy:.4f} - Test Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "OQ8lY-UF9vO9",
        "outputId": "270bd5fc-624b-4ef4-d0cc-805b18fb8417"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64, 73])\n",
            "Batch 0 labels: tensor([3., 3., 2., 0., 1., 1., 2., 3., 1., 0., 1., 1., 3., 3., 2., 1., 3., 3.,\n",
            "        3., 1., 3., 1., 3., 1., 1., 1., 3., 2., 1., 1., 1., 2.])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-248-fff9b882c206>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i} labels: {labels}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backward pass and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (0)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the train and test accuracy and loss\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax1.plot(train_accuracies, label='Train')\n",
        "ax1.plot(val_accuracies, label='Test')\n",
        "ax1.set_title('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(train_losses, label='Train')\n",
        "ax2.plot(val_losses, label='Test')\n",
        "ax2.set_title('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plfnajk4-U9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}