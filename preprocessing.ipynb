{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ckiptagger import data_utils, WS, POS, NER, construct_dictionary\n",
    "from ckip_transformers import __version__\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
      "To: D:\\Code\\NTUT_Thesis\\data.zip\n",
      "100%|██████████| 1.88G/1.88G [07:36<00:00, 4.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "#下載CKIP Tagger斷詞模型，下載完了已經不必再載第二次。\n",
    "# data_utils.download_data_gdown(r\"D:\\Code\\NTUT_Thesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.2\n",
      "Initializing drivers ... WS\n",
      "Initializing drivers ... POS\n",
      "Initializing drivers ... NER\n",
      "Initializing drivers ... all done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#導入CKIP Transfomers\n",
    "# Show version\n",
    "print(__version__)\n",
    "\n",
    "# Initialize drivers\n",
    "print(\"Initializing drivers ... WS\")\n",
    "ws_driver = CkipWordSegmenter(model=\"albert-base\", device=-1)\n",
    "print(\"Initializing drivers ... POS\")\n",
    "pos_driver = CkipPosTagger(model=\"albert-base\", device=-1)\n",
    "print(\"Initializing drivers ... NER\")\n",
    "ner_driver = CkipNerChunker(model=\"albert-base\", device=-1)\n",
    "print(\"Initializing drivers ... all done\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('taiwan_charity_news_v3.csv', encoding='UTF-8', index_col='Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Media</th>\n",
       "      <th>Content</th>\n",
       "      <th>Temp_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017/1/9</td>\n",
       "      <td>視障弟月薪僅800？勞工局：是見習生獎金，不是薪資。</td>\n",
       "      <td>ETtoday_News</td>\n",
       "      <td>一名雙眼失明的闕小弟因為父親入獄、肢體障礙的母親也遠在高雄，只剩他和弟弟2人在北部生活，只好...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017/1/13</td>\n",
       "      <td>開發金，邀九企業挺公益。</td>\n",
       "      <td>UDN_Database</td>\n",
       "      <td>開發金控多年來透過旗下中華開發工銀文教基金會及凱基社會慈善基金會，辦理各項公益活動。「年度公...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017/3/3</td>\n",
       "      <td>月亮的孩子加強視力，把握黃金期。</td>\n",
       "      <td>UDN_Database</td>\n",
       "      <td>白化症患者體內缺乏黑色素，頭髮和皮膚雪白，因眼睛畏光、在夜間活動相對較舒適，印地安人稱他們為...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                       Title         Media  \\\n",
       "Index                                                        \n",
       "1       2017/1/9  視障弟月薪僅800？勞工局：是見習生獎金，不是薪資。  ETtoday_News   \n",
       "2      2017/1/13                開發金，邀九企業挺公益。  UDN_Database   \n",
       "3       2017/3/3            月亮的孩子加強視力，把握黃金期。  UDN_Database   \n",
       "\n",
       "                                                 Content Temp_label  \n",
       "Index                                                                \n",
       "1      一名雙眼失明的闕小弟因為父親入獄、肢體障礙的母親也遠在高雄，只剩他和弟弟2人在北部生活，只好...    neutral  \n",
       "2      開發金控多年來透過旗下中華開發工銀文教基金會及凱基社會慈善基金會，辦理各項公益活動。「年度公...    neutral  \n",
       "3      白化症患者體內缺乏黑色素，頭髮和皮膚雪白，因眼睛畏光、在夜間活動相對較舒適，印地安人稱他們為...    neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清理內文中的一些髒東西\n",
    "for txts in data['Content']:\n",
    "    data['Content'] = data['Content'].replace(' ', '').replace(',', '，').replace('(', '（').replace(')', '）').replace('……', '')\n",
    "#\n",
    "for txts in data['Title']:\n",
    "    data['Title'] = data['Title'].replace(' ', '').replace(',', '，').replace('(', '（').replace(')', '）').replace('……', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['「天下的媽媽都是一樣的」，誰說視障女性無法勝任媽媽的角色？\\n愛盲基金會表示，基金會成立近30年，看見無數視障家庭面臨的困難與需求，也見證他們的努力，不少許多視障女性也面臨職場與家庭兩頭燒的諸多困境，愛盲服務的其中一位視障媽媽林佳臻，在視障、單親、低收入戶等窘迫條件下，15年來，不但一手撐起整個家、帶大4個孩子，甚至考取多項證照，實現自身志趣。\\n愛盲基金會指出，在台灣領有視障手冊的近6萬人中，約87%的人屬於中途失明，林佳臻在15年前因生產過程併發子癇前症，造成視神經萎縮，雙眼視力值只剩0.3，視野中間有黑影擋住，當時身無分文，連辦理低收證明所需的工本費都湊不足。林佳臻說，當時全身上下連5塊錢都沒有，心想如果找到工作，一定要好好做。\\n為了照顧一對龍鳳胎及另兩名幼女，林佳臻用米湯混奶粉以節省開銷，並爭取成為公園清潔代賑工，剛開始打掃時，常跌得滿身傷，總掃不乾淨，曾氣餒到想要摔掉掃把，但為了孩子咬牙忍下，一支掃把拿了12年，終於熬到龍鳳胎兒女今年升高一，放學後會主動到公園幫媽媽一起打掃。\\n林佳臻說，眼盲並不可怕，心盲才是最可悲的，只要自己走出來，尋求可能的幫助，這也不可恥，她來到愛盲，學會使用白手杖，出門不再容易跌倒；還有可以放大500倍的放大鏡和擴視機，協助她讀書、考上證照。\\n愛盲基金會副董事長黃克綸表示，愛盲今年集結了七位視障媽媽的故事，希望透過真實故事，讓大家看見任何一位必須兼顧職場與家庭的母親，都是不容易的，更何況是一位視障母親？很榮幸林佳臻站出來分享親身經歷，讓大家看見視障母親令人超乎想像的強大能量！也呼籲大家能與愛盲一起支持她們，成為所有視障家庭的強力後盾。\\n愛盲基金會說，今年受疫情重創，募款陡降近3成，但視障服務的腳步卻不能一日稍停，呼籲社會各界善心人士用行動支持愛盲，一起成為全台灣逾6萬名視障者、逾百萬名低視能者的後援。']\n"
     ]
    }
   ],
   "source": [
    "#取Index#100的新聞稿出來檢查\n",
    "test_string =data['Content'].loc[data.index == 100].values\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#寫一個func去除某些詞性的詞彙，等同篩除停用詞。\n",
    "#或許可以考慮在這個func裡面手動加入篩除特定詞的詞典...\n",
    "def wordfilter(sentence_ws, sentence_pos):\n",
    "  short_with_pos = []\n",
    "  short_sentence = []\n",
    "  stop_pos = set(['Nep', 'Nh']) # 不保留的詞性包括指代定詞、 代名詞\n",
    "  for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "    # 只留名詞和動詞\n",
    "    is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "    # 去掉某些詞性\n",
    "    is_not_stop_pos = word_pos not in stop_pos\n",
    "    # 去除只有單個字的詞\n",
    "    # is_not_one_charactor = not (len(word_ws) == 1)\n",
    "    # 組成串列\n",
    "    # if is_N_or_V and is_not_stop_pos and is_not_one_charactor:\n",
    "    if is_N_or_V and is_not_stop_pos:\n",
    "      short_with_pos.append(f\"{word_ws}({word_pos})\")\n",
    "      short_sentence.append(f\"{word_ws}\")\n",
    "  return (\" \".join(short_sentence), \" \".join(short_with_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, {'愛盲': 1.0}), (4, {'庇護工場': 1.0}), (5, {'愛盲基金會': 2.0})]\n"
     ]
    }
   ],
   "source": [
    "#建立Tagger用專有詞典，用來調整權重；CKIP Transfomer好像不能調整權重...\n",
    "# word_to_weight = {\n",
    "#     \"愛盲基金會\": 2,\n",
    "#     \"庇護工場\": 1,\n",
    "#     \"愛盲\": 1,\n",
    "# }\n",
    "# dictionary = construct_dictionary(word_to_weight)\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['「', '天下', '的', '媽媽', '都', '是', '一樣', '的', '」', '，', '誰', '說', '視障', '女性', '無法', '勝任', '媽媽', '的', '角色', '？', '\\n', '愛盲基金會', '表示', '，', '基金會', '成立', '近', '30', '年', '，', '看見', '無數', '視障', '家庭', '面臨', '的', '困難', '與', '需求', '，', '也', '見證', '他們', '的', '努力', '，', '不少', '許多', '視障', '女性', '也', '面臨', '職場', '與', '家庭', '兩', '頭', '燒', '的', '諸多', '困境', '，', '愛盲', '服務', '的', '其中', '一', '位', '視障', '媽媽', '林佳臻', '，', '在', '視障', '、', '單親', '、', '低收入戶', '等', '窘迫', '條件', '下', '，', '15', '年', '來', '，', '不但', '一手', '撐起', '整', '個', '家', '、', '帶', '大', '4', '個', '孩子', '，', '甚至', '考取', '多', '項', '證照', '，', '實現', '自身', '志趣', '。', '\\n', '愛盲基金會', '指出', '，', '在', '台灣', '領有', '視障', '手冊', '的', '近', '6萬', '人', '中', '，', '約', '87%', '的', '人', '屬於', '中途', '失明', '，', '林佳臻', '在', '15', '年', '前', '因', '生產', '過程', '併發', '子癇前', '症', '，', '造成', '視神經', '萎縮', '，', '雙眼', '視力值', '只', '剩', '0.3', '，', '視野', '中間', '有', '黑影', '擋住', '，', '當時', '身', '無', '分文', '，', '連', '辦理', '低收', '證明', '所', '需', '的', '工本費', '都', '湊', '不足', '。', '林佳臻', '說', '，', '當時', '全身', '上下', '連', '5', '塊', '錢', '都', '沒有', '，', '心想', '如果', '找到', '工作', '，', '一定', '要', '好好', '做', '。', '\\n', '為', '了', '照顧', '一', '對', '龍鳳胎', '及', '另', '兩', '名', '幼女', '，', '林佳臻', '用', '米湯', '混', '奶粉', '以', '節省', '開銷', '，', '並', '爭取', '成為', '公園', '清潔', '代賑工', '，', '剛', '開始', '打掃', '時', '，', '常', '跌', '得', '滿', '身', '傷', '，', '總', '掃', '不', '乾淨', '，', '曾', '氣餒', '到', '想要', '摔掉', '掃把', '，', '但', '為了', '孩子', '咬牙', '忍下', '，', '一', '支', '掃把', '拿', '了', '12', '年', '，', '終於', '熬到', '龍鳳胎', '兒女', '今年', '升高一', '，', '放學', '後', '會', '主動', '到', '公園', '幫', '媽媽', '一起', '打掃', '。', '\\n', '林佳臻', '說', '，', '眼盲', '並', '不', '可怕', '，', '心盲', '才', '是', '最', '可悲', '的', '，', '只要', '自己', '走出來', '，', '尋求', '可能', '的', '幫助', '，', '這', '也', '不', '可恥', '，', '她', '來到', '愛盲', '，', '學會', '使用', '白手杖', '，', '出門', '不再', '容易', '跌倒', '；', '還', '有', '可以', '放大', '500', '倍', '的', '放大鏡', '和', '擴視機', '，', '協助', '她', '讀書', '、', '考上', '證照', '。', '\\n', '愛盲基金會', '副董事長', '黃克綸', '表示', '，', '愛盲', '今年', '集結', '了', '七', '位', '視障', '媽媽', '的', '故事', '，', '希望', '透過', '真實', '故事', '，', '讓', '大家', '看見', '任何', '一', '位', '必須', '兼顧', '職場', '與', '家庭', '的', '母親', '，', '都', '是', '不', '容易', '的', '，', '更何況', '是', '一', '位', '視障', '母親', '？', '很', '榮幸', '林佳臻', '站出來', '分享', '親身', '經歷', '，', '讓', '大家', '看見', '視障', '母親', '令', '人', '超乎', '想像', '的', '強大', '能量', '！', '也', '呼籲', '大家', '能', '與', '愛盲', '一起', '支持', '她們', '，', '成為', '所有', '視障', '家庭', '的', '強力', '後盾', '。', '\\n', '愛盲基金會', '說', '，', '今年', '受', '疫情', '重創', '，', '募款', '陡降', '近', '3成', '，', '但', '視障', '服務', '的', '腳步', '卻', '不能', '一', '日', '稍', '停', '，', '呼籲', '社會', '各界', '善心', '人士', '用', '行動', '支持', '愛盲', '，', '一起', '成為', '全', '台灣', '逾', '6萬', '名', '視障者', '、', '逾', '百萬', '名', '低視能', '者', '的', '後援', '。']]\n"
     ]
    }
   ],
   "source": [
    "#Tagger斷詞器測試\n",
    "# ws_test = WS(\"./data/\", disable_cuda=True)\n",
    "\n",
    "# word_sentence_list = ws_test(\n",
    "#     test_string, \n",
    "#     sentence_segmentation = True, \n",
    "#     segment_delimiter_set = {\"，\", \"。\", \"！\", \"？\", \"：\", \"；\", \"…\", \"、\"}, \n",
    "#     recommend_dictionary = dictionary, \n",
    "#     )\n",
    "# print(word_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 297.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 200.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斷詞後：\n",
      "天下 媽媽 一樣 說 視障 女性 勝任 媽媽 角色 愛盲 基金會 表示 基金會 成立 近 30 年 看見 無數 視障 家庭 面臨 困難 需求 見證 努力 不少 許多 視障 女性 面臨 職場 家庭 兩 頭 燒 諸多 困境 愛 盲 服務 一 位 視障 媽媽 林佳臻 視障 單親 低收入戶 窘迫 條件 下 15 年 來 撐起 整 個 家 帶大 4 個 孩子 考取 多 項 證照 實現 志趣 愛盲 基金會 指出 台灣 領有 視障 手冊 近 6萬 人 中 87% 人 屬於 中途 失明 林佳臻 15 年 前 生產 過程 併發 子 癇前症 造成 視神經 萎縮 雙眼 視力值 剩 0.3 視野 中間 有 黑影 擋住 當時 身 無 分文 辦理 低收 證明 需 工本費 湊 不足 林佳臻 說 當時 全身 上下 5 塊 錢 沒有 心想 找到 工作 做 照顧 一 對 龍鳳胎 另 兩 名 幼女 林佳臻 米湯 混 奶粉 節省 開銷 爭取 成為 公園 清潔 代賑工 開始 打掃 時 跌 滿 身 傷 掃 乾淨 氣餒 想要 摔掉 掃把 孩子 咬牙 忍下 一 支 掃把 拿 12 年 熬到 龍鳳胎 兒女 今年 升高 一 放學 後 主動 到 公園 媽媽 打掃 林佳臻 說 眼盲 可怕 心盲 可悲 走出來 尋求 可能 幫助 可恥 來到 愛 盲 學會 使用 白 手杖 出門 容 易 跌倒 有 放大 500 倍 放大鏡 擴視機 協助 讀書 考上 證照 愛盲 基金會 副董事長 黃克綸 表示 愛盲 今年 集結 七 位 視障 媽媽 故事 希望 真實 故事 讓 看見 任何 一 位 兼顧 職場 家庭 母親 容易 一 位 視障 母親 榮幸 林佳臻 站出來 分享 經歷 讓 看見 視障 母親 令 人 超乎 想像 強大 能量 呼籲 愛盲 支持 成為 所有 視障 家庭 後盾 愛盲 基金會 說 今年 疫情 重創 募款 陡降 3成 視障 服務 腳步 一 日 停 呼籲 社會 各界 善心 人士 行動 支持 愛盲 成為 全 台灣 逾 6萬 名 視障者 逾 百萬 名 低視能 者 後援\n",
      "斷詞後+詞性標注：\n",
      "天下(Nc) 媽媽(Na) 一樣(VH) 說(VE) 視障(VH) 女性(Na) 勝任(VJ) 媽媽(Na) 角色(Na) 愛盲(VL) 基金會(Nc) 表示(VE) 基金會(Nc) 成立(VC) 近(Nes) 30(Neu) 年(Nf) 看見(VE) 無數(Neqa) 視障(VH) 家庭(Na) 面臨(VK) 困難(Na) 需求(Na) 見證(VE) 努力(Nv) 不少(Neqa) 許多(Neqa) 視障(VH) 女性(Na) 面臨(VK) 職場(Nc) 家庭(Na) 兩(Neu) 頭(Ncd) 燒(VC) 諸多(Neqa) 困境(Na) 愛(VL) 盲(VH) 服務(VC) 一(Neu) 位(Nf) 視障(VH) 媽媽(Na) 林佳臻(Nb) 視障(VH) 單親(Na) 低收入戶(Na) 窘迫(VH) 條件(Na) 下(Ng) 15(Neu) 年(Nf) 來(Ng) 撐起(VC) 整(Neqa) 個(Nf) 家(Nc) 帶大(VC) 4(Neu) 個(Nf) 孩子(Na) 考取(VJ) 多(Neqa) 項(Nf) 證照(Na) 實現(VC) 志趣(Na) 愛盲(VL) 基金會(Nc) 指出(VE) 台灣(Nc) 領有(VJ) 視障(VH) 手冊(Na) 近(Nes) 6萬(Neu) 人(Na) 中(Ng) 87%(Neqa) 人(Na) 屬於(VG) 中途(Nc) 失明(VH) 林佳臻(Nb) 15(Neu) 年(Nf) 前(Ng) 生產(VC) 過程(Na) 併發(VC) 子(Na) 癇前症(Na) 造成(VK) 視神經(Na) 萎縮(VH) 雙眼(Na) 視力值(Na) 剩(VJ) 0.3(Neu) 視野(Na) 中間(Ncd) 有(V_2) 黑影(Na) 擋住(VC) 當時(Nd) 身(Na) 無(VJ) 分文(Na) 辦理(VC) 低收(Na) 證明(Na) 需(VK) 工本費(Na) 湊(VC) 不足(VH) 林佳臻(Nb) 說(VE) 當時(Nd) 全身(Neqa) 上下(Ncd) 5(Neu) 塊(Nf) 錢(Na) 沒有(VJ) 心想(VE) 找到(VC) 工作(Na) 做(VC) 照顧(VC) 一(Neu) 對(Nf) 龍鳳胎(Na) 另(Nes) 兩(Neu) 名(Nf) 幼女(Na) 林佳臻(Nb) 米湯(Na) 混(VC) 奶粉(Na) 節省(VJ) 開銷(Na) 爭取(VC) 成為(VG) 公園(Nc) 清潔(VH) 代賑工(Na) 開始(VL) 打掃(VC) 時(Ng) 跌(VA) 滿(Neqa) 身(Na) 傷(Na) 掃(VC) 乾淨(VH) 氣餒(VH) 想要(VE) 摔掉(VC) 掃把(Na) 孩子(Na) 咬牙(VA) 忍下(VJ) 一(Neu) 支(Nf) 掃把(Na) 拿(VC) 12(Neu) 年(Nf) 熬到(VJ) 龍鳳胎(Na) 兒女(Na) 今年(Nd) 升高(VG) 一(Neu) 放學(VH) 後(Ng) 主動(VH) 到(VCL) 公園(Nc) 媽媽(Na) 打掃(VC) 林佳臻(Nb) 說(VE) 眼盲(Na) 可怕(VH) 心盲(Na) 可悲(VH) 走出來(VA) 尋求(VC) 可能(VH) 幫助(Na) 可恥(VH) 來到(VCL) 愛(VL) 盲(VH) 學會(Nc) 使用(VC) 白(VH) 手杖(Na) 出門(VA) 容(VH) 易(VH) 跌倒(VA) 有(V_2) 放大(VC) 500(Neu) 倍(Na) 放大鏡(Na) 擴視機(Na) 協助(VC) 讀書(VA) 考上(VC) 證照(Na) 愛盲(Nb) 基金會(Nc) 副董事長(Na) 黃克綸(Nb) 表示(VE) 愛盲(VL) 今年(Nd) 集結(VAC) 七(Neu) 位(Nf) 視障(VH) 媽媽(Na) 故事(Na) 希望(VK) 真實(VH) 故事(Na) 讓(VL) 看見(VE) 任何(Neqa) 一(Neu) 位(Nf) 兼顧(VC) 職場(Nc) 家庭(Na) 母親(Na) 容易(VH) 一(Neu) 位(Nf) 視障(VH) 母親(Na) 榮幸(VH) 林佳臻(Nb) 站出來(VA) 分享(VJ) 經歷(VJ) 讓(VL) 看見(VE) 視障(VH) 母親(Na) 令(VL) 人(Na) 超乎(VJ) 想像(Na) 強大(VH) 能量(Na) 呼籲(VE) 愛盲(Na) 支持(VC) 成為(VG) 所有(Neqa) 視障(VH) 家庭(Na) 後盾(Na) 愛盲(VL) 基金會(Nc) 說(VE) 今年(Nd) 疫情(Na) 重創(VC) 募款(Na) 陡降(VC) 3成(Neqa) 視障(VH) 服務(VC) 腳步(Na) 一(Neu) 日(Nf) 停(VHC) 呼籲(VE) 社會(Na) 各界(Na) 善心(Na) 人士(Na) 行動(Na) 支持(VC) 愛盲(VL) 成為(VG) 全(Neqa) 台灣(Nc) 逾(VJ) 6萬(Neu) 名(Nf) 視障者(VH) 逾(VJ) 百萬(Neu) 名(Nf) 低視能(VH) 者(Na) 後援(Na)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#CKIP Transfomer分詞、詞性標註、命名實體識別\n",
    "ws = ws_driver(test_string)\n",
    "pos = pos_driver(ws)\n",
    "ner = ner_driver(test_string)\n",
    "for sentence, sentence_ws, sentence_pos, sentence_ner in zip(test_string, ws, pos, ner):\n",
    "    (short, res) = wordfilter(sentence_ws, sentence_pos)\n",
    "    print(\"斷詞後：\")\n",
    "    print(short)\n",
    "    print(\"斷詞後+詞性標注：\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['視障', '愛盲', '林佳臻', '基金會', '媽媽', '家庭', '成為', '看見', '今年', '母親', '表示', '支持', '女性', '龍鳳胎', '故事']\n",
      "(1, 161)\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Keyword Extraction TF-IDF Method\n",
    "# 建立 CountVectorizer 物件\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 將 text 轉成詞頻矩陣\n",
    "word_count = vectorizer.fit_transform([short])\n",
    "\n",
    "# 建立 TfidfTransformer 物件\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# 將詞頻矩陣轉換成 TF-IDF 矩陣\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(word_count)\n",
    "\n",
    "# 取得詞彙表\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# 取得每個詞彙的 TF-IDF 值\n",
    "tfidf_values = tfidf_matrix.toarray()[0]\n",
    "\n",
    "# 取得排序後的索引位置\n",
    "sorted_index = np.argsort(tfidf_values)[::-1]\n",
    "\n",
    "# 取得排名前 K位 的關鍵字\n",
    "top_k = 15\n",
    "tfidf_keywords = [words[i] for i in sorted_index[:top_k]]\n",
    "\n",
    "print(tfidf_keywords)\n",
    "print(tfidf_matrix.shape)\n",
    "print(len(tfidf_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3成', '今年', '台灣', '單親', '林佳臻', '讀書', '12 年', '15 年', '3', '4', '5', '500', '6', '87%', '七']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Keyword Extraction TextRank Method\n",
    "# 要確認有先在環境執行python -m spacy download zh_core_web_sm 下載中文模型\n",
    "\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "doc = nlp(short)\n",
    "\n",
    "# extract the top 10 keywords\n",
    "textrank_keywords = []\n",
    "for p in doc._.phrases:\n",
    "    if len(textrank_keywords) >= 15:\n",
    "        break\n",
    "    textrank_keywords.append(p.text)\n",
    "    \n",
    "print(textrank_keywords)\n",
    "print(len(textrank_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: 視障 愛盲 林佳臻 基金會 媽媽 家庭 成為 看見 今年 母親 表示 支持 女性 龍鳳胎 故事\n"
     ]
    }
   ],
   "source": [
    "# Keyword Extraction LDA Method\n",
    "# 設定LDA模型的參數\n",
    "NUM_TOPICS = 1 #看要設定幾個主題\n",
    "NUM_KEYWORDS = 15\n",
    "MAX_ITER = 50\n",
    "\n",
    "# 建立CountVectorizer物件，並使用它將文本轉換成詞頻矩陣\n",
    "# vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([short])\n",
    "\n",
    "# 建立LDA模型，並使用fit_transform方法將詞頻矩陣轉換成主題-詞頻分布矩陣\n",
    "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=MAX_ITER)\n",
    "lda.fit_transform(X)\n",
    "\n",
    "# 取出每個主題的關鍵字\n",
    "lda_keywords = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_keyword_idxs = topic.argsort()[:-NUM_KEYWORDS-1:-1]\n",
    "    top_keywords = [vectorizer.get_feature_names()[idx] for idx in top_keyword_idxs]\n",
    "    lda_keywords.append(top_keywords)\n",
    "\n",
    "# 顯示每個主題的關鍵字\n",
    "for i, topic_keywords in enumerate(lda_keywords):\n",
    "    print(f\"Topic #{i+1}: {' '.join(topic_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['視障', '愛盲', '林佳臻', '基金會', '媽媽', '家庭', '成為', '看見', '今年', '母親', '表示', '支持', '女性', '龍鳳胎', '故事']\n"
     ]
    }
   ],
   "source": [
    "print(lda_keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 變數有三種:\n",
    "# tfidf_keywords\n",
    "# textrank_keywords\n",
    "# lda_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文維基詞向量-路徑\n",
    "zhwiki_path = r'D:\\Code\\NTUT_Thesis\\model_data\\zhwiki\\wiki.zh.vector'\n",
    "#fastText Facebook 20萬字訓練詞向量-路徑\n",
    "fasttext_path = r'D:\\Code\\NTUT_Thesis\\model_data\\fastText_cbow_300d_facebook\\cc.zh.300.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入詞向量function\n",
    "def load_fasttext():\n",
    "        print('loading word embeddings...')\n",
    "        embeddings_index = {}\n",
    "        f = open(r'D:\\Code\\NTUT_Thesis\\model_data\\fastText_cbow_300d_facebook\\cc.zh.300.vec', encoding='utf-8')\n",
    "        for line in tqdm(f):\n",
    "                values = line.strip().rsplit(' ')\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print('found %s word vectors' % len(embeddings_index))\n",
    "        return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [03:41, 9036.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2000000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 載入詞向量\n",
    "model = load_fasttext()\n",
    "#model的資料結構是dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-1.5040e-01 -3.9000e-01  2.6000e-01  2.7670e-01 -2.1450e-01 -7.6100e-02\n",
      " -5.9200e-01 -1.6080e-01  3.7380e-01  2.9700e-01  9.2000e-02  4.2750e-01\n",
      " -2.8560e-01 -6.5800e-02  2.0620e-01 -3.4460e-01  2.8060e-01 -2.0880e-01\n",
      "  2.2100e-01 -2.4500e-02  2.1840e-01 -1.8780e-01  1.8080e-01 -3.0930e-01\n",
      " -5.4700e-02  2.9680e-01  7.9600e-02  1.6700e-01  8.8300e-02  2.7000e-01\n",
      " -1.3920e-01  6.7300e-02  3.4120e-01 -3.5320e-01 -6.5930e-01  2.0300e-01\n",
      " -3.1740e-01  4.7500e-02 -1.3100e-01 -9.4400e-02  7.3500e-02 -8.5100e-02\n",
      " -1.0800e-01 -1.8140e-01 -2.7840e-01  2.1710e-01 -2.7060e-01  6.8100e-02\n",
      "  9.8900e-02 -3.2600e-02  2.7010e-01 -2.7040e-01 -5.4900e-02  2.4130e-01\n",
      " -4.2800e-02 -2.6310e-01  8.9400e-02 -2.7600e-02 -2.2090e-01  4.8440e-01\n",
      " -2.1410e-01  8.2700e-02  1.1580e-01 -3.0720e-01 -2.9490e-01 -3.1590e-01\n",
      "  2.6500e-02  4.1790e-01  7.0990e-01  1.3640e-01 -3.5080e-01  1.2620e-01\n",
      "  2.0900e-01 -3.8500e-02  4.0610e-01  1.5360e-01  1.1680e-01 -9.1400e-02\n",
      "  1.8170e-01  1.0420e-01 -6.1340e-01  5.2500e-02  3.9700e-02  5.9840e-01\n",
      " -2.2070e-01  1.3680e-01  3.3170e-01  3.3080e-01  2.5210e-01 -1.7970e-01\n",
      "  2.3970e-01 -2.0890e-01 -3.6050e-01 -3.5900e-02  6.1400e-02 -4.2120e-01\n",
      " -8.0100e-02 -6.0340e-01 -2.1260e-01 -1.1270e-01 -2.0460e-01  1.7550e-01\n",
      "  1.8530e-01  8.9400e-02 -6.6700e-02  1.4530e-01 -6.2600e-02 -3.2500e-02\n",
      "  1.2990e-01 -1.8800e-02  3.3700e-02 -3.9500e-02 -3.6190e-01  4.6070e-01\n",
      " -9.4600e-02 -7.7100e-02 -4.7000e-03  2.6870e-01  3.4600e-02 -9.0800e-02\n",
      "  3.0890e-01  2.0000e-04  3.9300e-02 -3.9000e-03  5.8550e-01 -1.0900e-02\n",
      "  5.1900e-02 -2.4220e-01 -2.3080e-01 -7.8600e-02 -4.5400e-01  3.6040e-01\n",
      " -5.7880e-01  4.2570e-01  1.6090e-01  3.6000e-03 -1.7020e-01  4.8260e-01\n",
      " -3.5460e-01 -9.8000e-02 -1.8550e-01  2.5400e-02 -1.8900e-02  2.7100e-02\n",
      " -5.6000e-03 -2.6680e-01  9.7200e-02 -2.0870e-01  2.4840e-01  1.5170e-01\n",
      " -3.3710e-01  4.9290e-01 -1.1720e-01  1.6700e-02  4.9210e-01 -3.5800e-02\n",
      "  3.0500e-02 -5.6900e-02  1.1190e-01  1.2090e-01 -1.3630e-01 -1.3000e-01\n",
      "  2.4110e-01 -9.5300e-02 -2.3760e-01  1.5000e-01  2.1300e-01 -7.0250e-01\n",
      "  5.0200e-02  2.8500e-02 -1.8700e-01  3.5290e-01 -2.8420e-01  5.8000e-02\n",
      "  1.6500e-01 -6.6000e-02 -4.8630e-01  8.9800e-02  1.2970e-01 -2.5880e-01\n",
      " -5.1550e-01 -4.8900e-02 -4.5110e-01  8.5000e-02 -4.5720e-01  2.0890e-01\n",
      " -1.4910e-01  4.5700e-01  1.4000e-03  5.3130e-01 -3.1410e-01 -5.9430e-01\n",
      "  5.6160e-01  1.1460e-01  2.7600e-02 -5.3170e-01  2.4950e-01 -1.1410e-01\n",
      "  7.2300e-02 -5.4900e-02  9.0300e-02 -2.6610e-01 -9.3000e-02 -3.0460e-01\n",
      " -5.8500e-02  2.4280e-01  6.8940e-01  1.3930e-01  6.6000e-02  2.0510e-01\n",
      " -1.9320e-01 -2.6200e-02 -3.2700e-02 -2.8000e-01 -3.3110e-01 -4.5730e-01\n",
      "  1.8170e-01 -8.0600e-02 -3.6740e-01 -3.0330e-01  5.0170e-01 -4.9900e-01\n",
      " -1.7200e-02  1.9490e-01 -3.2260e-01  1.2530e-01  4.2800e-02  3.0340e-01\n",
      " -1.7600e-02 -6.8060e-01 -1.3410e-01  1.2796e+00  4.4440e-01 -9.0400e-02\n",
      " -1.9800e-02 -1.1100e-01  1.0950e-01  2.4020e-01 -3.9420e-01 -2.2520e-01\n",
      "  1.1230e-01  1.7960e-01 -3.5300e-01  1.0340e-01  3.9700e-02 -1.1300e-02\n",
      " -1.7000e-01  2.1600e-02 -1.3710e-01 -1.7790e-01  1.1080e-01 -2.1800e-01\n",
      "  1.3300e-02  4.7490e-01  2.6780e-01 -2.2480e-01  4.0300e-02  1.4460e-01\n",
      "  1.1230e-01 -1.3510e-01  9.4900e-02  2.0610e-01  1.2800e-01  8.9700e-02\n",
      "  4.2220e-01  3.4040e-01  2.0810e-01  9.5000e-03  8.3900e-02  1.6510e-01\n",
      " -1.2720e-01  2.0530e-01  1.2510e-01 -1.5190e-01  3.2800e-02  3.3800e-01\n",
      " -2.4270e-01 -4.9000e-02 -1.2630e-01  7.4100e-02  2.6500e-02 -1.8050e-01\n",
      "  6.2200e-02  5.8700e-01 -1.2710e-01  5.2820e-01 -2.2990e-01  1.0310e-01\n",
      " -8.5900e-02 -1.8250e-01 -3.4990e-01  7.3000e-03  2.4790e-01 -8.9200e-02\n",
      "  3.3840e-01  3.2760e-01 -2.3240e-01 -5.1600e-02 -2.3610e-01 -9.9700e-02]\n"
     ]
    }
   ],
   "source": [
    "vec = model['視障']\n",
    "print(vec.shape)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 若未知詞使用『全零向量』填充，這邊製作全零向量\n",
    "vec_size = next(iter(model.items()))[1]\n",
    "unknown = np.zeros((vec_size.shape), dtype=np.float32)\n",
    "print(unknown.shape)\n",
    "print(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[-1.5040e-01 -3.9000e-01  2.6000e-01  2.7670e-01 -2.1450e-01 -7.6100e-02\n",
      " -5.9200e-01 -1.6080e-01  3.7380e-01  2.9700e-01  9.2000e-02  4.2750e-01\n",
      " -2.8560e-01 -6.5800e-02  2.0620e-01 -3.4460e-01  2.8060e-01 -2.0880e-01\n",
      "  2.2100e-01 -2.4500e-02  2.1840e-01 -1.8780e-01  1.8080e-01 -3.0930e-01\n",
      " -5.4700e-02  2.9680e-01  7.9600e-02  1.6700e-01  8.8300e-02  2.7000e-01\n",
      " -1.3920e-01  6.7300e-02  3.4120e-01 -3.5320e-01 -6.5930e-01  2.0300e-01\n",
      " -3.1740e-01  4.7500e-02 -1.3100e-01 -9.4400e-02  7.3500e-02 -8.5100e-02\n",
      " -1.0800e-01 -1.8140e-01 -2.7840e-01  2.1710e-01 -2.7060e-01  6.8100e-02\n",
      "  9.8900e-02 -3.2600e-02  2.7010e-01 -2.7040e-01 -5.4900e-02  2.4130e-01\n",
      " -4.2800e-02 -2.6310e-01  8.9400e-02 -2.7600e-02 -2.2090e-01  4.8440e-01\n",
      " -2.1410e-01  8.2700e-02  1.1580e-01 -3.0720e-01 -2.9490e-01 -3.1590e-01\n",
      "  2.6500e-02  4.1790e-01  7.0990e-01  1.3640e-01 -3.5080e-01  1.2620e-01\n",
      "  2.0900e-01 -3.8500e-02  4.0610e-01  1.5360e-01  1.1680e-01 -9.1400e-02\n",
      "  1.8170e-01  1.0420e-01 -6.1340e-01  5.2500e-02  3.9700e-02  5.9840e-01\n",
      " -2.2070e-01  1.3680e-01  3.3170e-01  3.3080e-01  2.5210e-01 -1.7970e-01\n",
      "  2.3970e-01 -2.0890e-01 -3.6050e-01 -3.5900e-02  6.1400e-02 -4.2120e-01\n",
      " -8.0100e-02 -6.0340e-01 -2.1260e-01 -1.1270e-01 -2.0460e-01  1.7550e-01\n",
      "  1.8530e-01  8.9400e-02 -6.6700e-02  1.4530e-01 -6.2600e-02 -3.2500e-02\n",
      "  1.2990e-01 -1.8800e-02  3.3700e-02 -3.9500e-02 -3.6190e-01  4.6070e-01\n",
      " -9.4600e-02 -7.7100e-02 -4.7000e-03  2.6870e-01  3.4600e-02 -9.0800e-02\n",
      "  3.0890e-01  2.0000e-04  3.9300e-02 -3.9000e-03  5.8550e-01 -1.0900e-02\n",
      "  5.1900e-02 -2.4220e-01 -2.3080e-01 -7.8600e-02 -4.5400e-01  3.6040e-01\n",
      " -5.7880e-01  4.2570e-01  1.6090e-01  3.6000e-03 -1.7020e-01  4.8260e-01\n",
      " -3.5460e-01 -9.8000e-02 -1.8550e-01  2.5400e-02 -1.8900e-02  2.7100e-02\n",
      " -5.6000e-03 -2.6680e-01  9.7200e-02 -2.0870e-01  2.4840e-01  1.5170e-01\n",
      " -3.3710e-01  4.9290e-01 -1.1720e-01  1.6700e-02  4.9210e-01 -3.5800e-02\n",
      "  3.0500e-02 -5.6900e-02  1.1190e-01  1.2090e-01 -1.3630e-01 -1.3000e-01\n",
      "  2.4110e-01 -9.5300e-02 -2.3760e-01  1.5000e-01  2.1300e-01 -7.0250e-01\n",
      "  5.0200e-02  2.8500e-02 -1.8700e-01  3.5290e-01 -2.8420e-01  5.8000e-02\n",
      "  1.6500e-01 -6.6000e-02 -4.8630e-01  8.9800e-02  1.2970e-01 -2.5880e-01\n",
      " -5.1550e-01 -4.8900e-02 -4.5110e-01  8.5000e-02 -4.5720e-01  2.0890e-01\n",
      " -1.4910e-01  4.5700e-01  1.4000e-03  5.3130e-01 -3.1410e-01 -5.9430e-01\n",
      "  5.6160e-01  1.1460e-01  2.7600e-02 -5.3170e-01  2.4950e-01 -1.1410e-01\n",
      "  7.2300e-02 -5.4900e-02  9.0300e-02 -2.6610e-01 -9.3000e-02 -3.0460e-01\n",
      " -5.8500e-02  2.4280e-01  6.8940e-01  1.3930e-01  6.6000e-02  2.0510e-01\n",
      " -1.9320e-01 -2.6200e-02 -3.2700e-02 -2.8000e-01 -3.3110e-01 -4.5730e-01\n",
      "  1.8170e-01 -8.0600e-02 -3.6740e-01 -3.0330e-01  5.0170e-01 -4.9900e-01\n",
      " -1.7200e-02  1.9490e-01 -3.2260e-01  1.2530e-01  4.2800e-02  3.0340e-01\n",
      " -1.7600e-02 -6.8060e-01 -1.3410e-01  1.2796e+00  4.4440e-01 -9.0400e-02\n",
      " -1.9800e-02 -1.1100e-01  1.0950e-01  2.4020e-01 -3.9420e-01 -2.2520e-01\n",
      "  1.1230e-01  1.7960e-01 -3.5300e-01  1.0340e-01  3.9700e-02 -1.1300e-02\n",
      " -1.7000e-01  2.1600e-02 -1.3710e-01 -1.7790e-01  1.1080e-01 -2.1800e-01\n",
      "  1.3300e-02  4.7490e-01  2.6780e-01 -2.2480e-01  4.0300e-02  1.4460e-01\n",
      "  1.1230e-01 -1.3510e-01  9.4900e-02  2.0610e-01  1.2800e-01  8.9700e-02\n",
      "  4.2220e-01  3.4040e-01  2.0810e-01  9.5000e-03  8.3900e-02  1.6510e-01\n",
      " -1.2720e-01  2.0530e-01  1.2510e-01 -1.5190e-01  3.2800e-02  3.3800e-01\n",
      " -2.4270e-01 -4.9000e-02 -1.2630e-01  7.4100e-02  2.6500e-02 -1.8050e-01\n",
      "  6.2200e-02  5.8700e-01 -1.2710e-01  5.2820e-01 -2.2990e-01  1.0310e-01\n",
      " -8.5900e-02 -1.8250e-01 -3.4990e-01  7.3000e-03  2.4790e-01 -8.9200e-02\n",
      "  3.3840e-01  3.2760e-01 -2.3240e-01 -5.1600e-02 -2.3610e-01 -9.9700e-02]\n",
      "['林佳臻']\n"
     ]
    }
   ],
   "source": [
    "#好...我們現在有一組keyword存在List裡面\n",
    "#['視障', '愛盲', '林佳臻', '基金會', '媽媽', '家庭', '成為', '看見', '今年', '母親', '表示', '支持', '女性', '龍鳳胎', '故事']\n",
    "#那麼我們把這個List裡面的所有元素依序丟入model裡面，每個元素會被換算出一組300維的向量。\n",
    "#那個(300,0)x15的向量該怎麼儲存比較好呢？List嗎?這樣就會是一個由15組array為元素的List，是這樣嗎??\n",
    "#對了，某些專有名詞或人名沒有被訓練在word vector裡面...所以...要怎麼辦呢?\n",
    "#某些討論有提到，(1)未知詞使用『模型平均向量』填充  (2)未知詞使用『全零向量』填充\n",
    "#假設是這樣的話就這樣寫:\n",
    "\n",
    "ldavec_list = [] #由15組詞向量作為元素的List\n",
    "unknown_list = [] #把未知詞都另外添加到其他List，事後回來檢查有沒有重要詞是未知詞\n",
    "\n",
    "for i in lda_keywords[0]:\n",
    "    if i in model.keys(): #檢查詞有沒有被訓練在模型裡面，沒有就是未知詞\n",
    "        ldavec_list.append(model[i])\n",
    "    else:\n",
    "        ldavec_list.append(unknown)\n",
    "        unknown_list.append(i)\n",
    "print(len(ldavec_list)) #檢查由15組詞向量作為元素的List的長度\n",
    "print(ldavec_list[0]) #檢查第1個向量\n",
    "print(unknown_list) #檢查未知詞名單\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
