{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文維基詞向量-路徑\n",
    "zhwiki_path = r'D:\\Code\\NTUT_Thesis\\model_data\\zhwiki\\wiki.zh.vector'\n",
    "#fastText Facebook 20萬字訓練詞向量-路徑\n",
    "fasttext_path = r'D:\\Code\\NTUT_Thesis\\model_data\\fastText_cbow_300d_facebook\\cc.zh.300.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入詞向量function\n",
    "def load_fasttext():\n",
    "        print('loading word embeddings...')\n",
    "        embeddings_index = {}\n",
    "        f = open(r'D:\\Code\\NTUT_Thesis\\model_data\\fastText_cbow_300d_facebook\\cc.zh.300.vec', encoding='utf-8')\n",
    "        for line in tqdm(f):\n",
    "                values = line.strip().rsplit(' ')\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print('found %s word vectors' % len(embeddings_index))\n",
    "        return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入詞向量\n",
    "model = load_fasttext()\n",
    "#model的資料結構是dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model['視障']\n",
    "print(vec.shape)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若未知詞使用『全零向量』填充，這邊製作全零向量\n",
    "vec_size = next(iter(model.items()))[1]\n",
    "unknown = np.zeros((vec_size.shape), dtype=np.float32)\n",
    "print(unknown.shape)\n",
    "print(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#好...我們現在有一組keyword存在List裡面\n",
    "#['視障', '愛盲', '林佳臻', '基金會', '媽媽', '家庭', '成為', '看見', '今年', '母親', '表示', '支持', '女性', '龍鳳胎', '故事']\n",
    "#那麼我們把這個List裡面的所有元素依序丟入model裡面，每個元素會被換算出一組300維的向量。\n",
    "#那個(300,0)x15的向量該怎麼儲存比較好呢？List嗎?這樣就會是一個由15組array為元素的List，是這樣嗎??\n",
    "#對了，某些專有名詞或人名沒有被訓練在word vector裡面...所以...要怎麼辦呢?\n",
    "#某些討論有提到，(1)未知詞使用『模型平均向量』填充  (2)未知詞使用『全零向量』填充\n",
    "#假設是這樣的話就這樣寫:\n",
    "\n",
    "ldavec_list = [] #由15組詞向量作為元素的List\n",
    "unknown_list = [] #把未知詞都另外添加到其他List，事後回來檢查有沒有重要詞是未知詞\n",
    "\n",
    "for i in lda_keywords[0]:\n",
    "    if i in model.keys(): #檢查詞有沒有被訓練在模型裡面，沒有就是未知詞\n",
    "        ldavec_list.append(model[i])\n",
    "    else:\n",
    "        ldavec_list.append(unknown)\n",
    "        unknown_list.append(i)\n",
    "print(len(ldavec_list)) #檢查由15組詞向量作為元素的List的長度\n",
    "print(ldavec_list[0]) #檢查第1個向量\n",
    "print(unknown_list) #檢查未知詞名單"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
