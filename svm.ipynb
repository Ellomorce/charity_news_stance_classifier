{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC \n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define File Path\n",
    "vec20avg_path = \"./vec_data/vec20_avg.npz\"\n",
    "vec25avg_path = \"./vec_data/vec25_avg.npz\"\n",
    "vec30avg_path = \"./vec_data/vec30_avg.npz\"\n",
    "vec35avg_path = \"./vec_data/vec35_avg.npz\"\n",
    "vec20sum_path = \"./vec_data/vec20_sum.npz\"\n",
    "vec20sum_path = \"./vec_data/vec20_sum.npz\"\n",
    "vec25sum_path = \"./vec_data/vec25_sum.npz\"\n",
    "vec30sum_path = \"./vec_data/vec30_sum.npz\"\n",
    "vec35sum_path = \"./vec_data/vec35_sum.npz\"\n",
    "freq_stance_labels = \"./vec_data/freq_stance_labels.npz\"\n",
    "oh_stance_labels = \"./vec_data/oh_stance_labels.npz\"\n",
    "le_stance_labels = \"./vec_data/le_stance_labels.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_file(filepath):\n",
    "    # Load the numpy array from the .npz file\n",
    "    with np.load(filepath, allow_pickle=True) as data:\n",
    "        for key in data.keys():\n",
    "            arr = data[key]\n",
    "            break\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec20avg = load_npz_file(vec20avg_path)\n",
    "# vec25avg = load_npz_file(vec25avg_path)\n",
    "vec30avg = load_npz_file(vec30avg_path)\n",
    "# vec35avg = load_npz_file(vec35avg_path)\n",
    "# vec20sum = load_npz_file(vec20sum_path)\n",
    "# vec20sum = load_npz_file(vec20sum_path)\n",
    "# vec25sum = load_npz_file(vec25sum_path)\n",
    "vec30sum = load_npz_file(vec30sum_path)\n",
    "# vec35sum = load_npz_file(vec35sum_path)\n",
    "freq_label = load_npz_file(freq_stance_labels)\n",
    "oh_label = load_npz_file(oh_stance_labels)\n",
    "le_label = load_npz_file(le_stance_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指派實際要使用的Data與Label\n",
    "# data = vec30avg\n",
    "data = vec30sum\n",
    "# label = np.argmax(oh_label, axis=1)\n",
    "# label = oh_label\n",
    "# label = freq_label\n",
    "label = le_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(np.unique(label))\n",
    "print(np.unique(label, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation using Oversampling\n",
    "# Define the oversampling ratio for each class\n",
    "oversample_ratio = {0: 100, 1: 201, 2: 120, 3: 159}\n",
    "# Initialize the oversampler\n",
    "oversampler = RandomOverSampler(sampling_strategy=oversample_ratio)\n",
    "# Reshape your data to a 2D matrix of shape (n_samples, n_features)\n",
    "X = data.reshape(-1, 300)\n",
    "# Apply oversampling to X and y\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, label)\n",
    "# Reshape X back to its original shape\n",
    "X_resampled = X_resampled.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_resampled.shape)\n",
    "# print(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_resampled.shape)\n",
    "# print(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using original data\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(data, label, test_size=0.2)\n",
    "# Using oversampling data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X_resampled, y_resampled, test_size=0.3)\n",
    "print('Train data shape:', train_data.shape)\n",
    "print('Train labels shape:', train_labels.shape)\n",
    "print('Test data shape:', test_data.shape)\n",
    "print('Test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best parameters with SVC\n",
    "kernels = list(['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "c = list([20,25,30,35,40,45,50])\n",
    "gammas = list([0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "\n",
    "clf = SVC()\n",
    "param_grid = dict(kernel=kernels, C=c, gamma=gammas)\n",
    "grid = GridSearchCV(clf, param_grid, cv=10, n_jobs=-1)\n",
    "grid.fit(train_data, train_labels)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=30, gamma=0.08, kernel='rbf') \n",
    "clf.fit(train_data, train_labels) \n",
    "\n",
    "y_pred = clf.predict(test_data)\n",
    "\n",
    "print('Accuracy Score: {:.4f}'.format(accuracy_score(test_labels, y_pred)))\n",
    "print('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, test_labels)))\n",
    "print('SVC precision : {:.4f}'.format(precision_score(y_pred, test_labels)))\n",
    "print('SVC recall    : {:.4f}'.format(recall_score(y_pred, test_labels)))\n",
    "print(\"\\n\",classification_report(y_pred, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_data)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_pred, test_labels)\n",
    "sns.heatmap((cf_matrix / np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a sanity check, let’s do 500 rounds of random sampling and assess the stability of our model:\n",
    "scores = [] \n",
    "for i in range(0,500): \n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(X_resampled, y_resampled, test_size=0.3)\n",
    "    clf = SVC(kernel='rbf', C=30, gamma=0.08) \n",
    "    clf.fit(train_data, train_labels)\n",
    "    scores.append(f1_score(clf.predict(test_data), test_labels)) \n",
    "\n",
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
